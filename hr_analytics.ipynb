{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cef06a-2c4e-4b07-abf4-0339b2cf0d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\henry\\anaconda3\\lib\\site-packages (4.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\henry\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ab845-c5e1-44c3-91b6-f1d8d192825b",
   "metadata": {},
   "source": [
    "<h3>Step 1: Start SparkSession</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e531be9e-834b-4558-8b33-de3f240a2fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HRAnalyticsPipline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x129706e6660>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"HRAnalyticsPipline\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a1c3f-b418-4d88-99bf-f5d749f8bfda",
   "metadata": {},
   "source": [
    "<h3>Step 2: Import Datasets for HR </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dbb1c0-1d3f-42eb-b3d1-13ba069c4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+---------+--------+--------------------+---------------+--------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+----------------+-------------------+-----------------------+-------------+\n",
      "|Unnamed: 0|FirstName|LastName|StartDate|ExitDate|               Title|     Supervisor|             ADEmail|BusinessUnit|EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|TerminationDescription|   DepartmentType|            Division|       DOB|State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance Score|Current Employee Rating|Employee ID|Survey Date|Engagement Score|Satisfaction Score|Work-Life Balance Score|Training Date|Training Program Name|Training Type|Training Outcome|        Location|            Trainer|Training Duration(Days)|Training Cost|\n",
      "+----------+---------+--------+---------+--------+--------------------+---------------+--------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+----------------+-------------------+-----------------------+-------------+\n",
      "|         0|    Uriah| Bridges|20-Sep-19|    NULL|Production Techni...|   Peter Oneill|uriah.bridges@bil...|        CCDR|        Active|    Contract| Zone C|                 Temporary|            Unk|                  NULL|Production       |Finance & Accounting|07-10-1969|   MA|            Accounting|    Female|       34904|   White|    Widowed|      Fully Meets|                      4|       3427| 14-01-2023|               1|                 2|                      3|    15-Jul-23| Leadership Develo...|     Internal|          Failed|    South Marisa|   Taylor Rodriguez|                      2|       606.11|\n",
      "|         1|    Paula|   Small|11-Feb-23|    NULL|Production Techni...|Renee Mccormick|paula.small@bilea...|          EW|        Active|    Contract| Zone A|                 Part-Time|            Unk|                  NULL|Production       |              Aerial|30-08-1965|   MA|                 Labor|      Male|        6593|Hispanic|    Widowed|      Fully Meets|                      3|       3428| 09-09-2022|               2|                 1|                      5|    12-Sep-22|     Customer Service|     External|      Incomplete|     Tammieville|Kelly Patterson DDS|                      4|       673.02|\n",
      "|         2|   Edward|    Buck|10-Dec-18|    NULL|  Area Sales Manager| Crystal Walker|edward.buck@bilea...|          PL|        Active|   Full-Time| Zone B|                 Part-Time|            Unk|                  NULL|            Sales|       General - Sga|06-10-1991|   MA|             Assistant|      Male|        2330|Hispanic|    Widowed|      Fully Meets|                      4|       3429| 27-05-2023|               1|                 2|                      1|    13-Aug-22| Leadership Develo...|     External|          Failed|East Roberthaven|      Taylor Thomas|                      2|       413.28|\n",
      "|         3|  Michael| Riordan|21-Jun-21|    NULL|  Area Sales Manager| Rebekah Wright|michael.riordan@b...|        CCDR|        Active|    Contract| Zone A|                 Full-Time|            Unk|                  NULL|            Sales|Finance & Accounting|04-04-1998|   ND|                 Clerk|      Male|       58782|   Other|     Single|      Fully Meets|                      2|       3430| 16-06-2023|               5|                 5|                      4|    15-Dec-22|   Project Management|     External|       Completed|       Garzatown|      Holly Elliott|                      3|       663.78|\n",
      "|         4|  Jasmine|   Onque|29-Jun-19|    NULL|  Area Sales Manager|      Jason Kim|jasmine.onque@bil...|         TNS|        Active|    Contract| Zone A|                 Temporary|            Unk|                  NULL|            Sales|       General - Con|29-08-1969|   FL|               Laborer|    Female|       33174|   Other|    Married|      Fully Meets|                      3|       3431| 25-11-2022|               2|                 5|                      3|    13-Jul-23|     Technical Skills|     External|          Failed| Lake Meganville|    Donald Martinez|                      5|       399.03|\n",
      "+----------+---------+--------+---------+--------+--------------------+---------------+--------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+----------------+-------------------+-----------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "hr_df = spark.read.csv(\"HR_dataset.csv\", header=True, inferSchema=True)\n",
    "hr_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6375395-93cd-4ed8-ab4f-79054ce0e235",
   "metadata": {},
   "source": [
    "<h3>Step 3: Perform cleaning for HR Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cdc1a0-53a3-460b-9e00-b681f54ffeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "|Unnamed: 0|FirstName|LastName|StartDate|ExitDate|Title|Supervisor|ADEmail|BusinessUnit|EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|TerminationDescription|DepartmentType|Division|DOB|State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance Score|Current Employee Rating|Employee ID|Survey Date|Engagement Score|Satisfaction Score|Work-Life Balance Score|Training Date|Training Program Name|Training Type|Training Outcome|Location|Trainer|Training Duration(Days)|Training Cost|\n",
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "|         0|        0|       0|        0|    1544|    0|         0|      0|           0|             0|           0|      0|                         0|              0|                  1544|             0|       0|  0|    0|                     0|         0|           0|       0|          0|                0|                      0|          0|          0|               0|                 0|                      0|            0|                    0|            0|               0|       0|      0|                      0|            0|\n",
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#a. view missing values\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "# Count nulls per column\n",
    "def missing_report(df):\n",
    "    return df.select([\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "missing_report(hr_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07c90f-4e27-455c-b8ae-8487913376b9",
   "metadata": {},
   "source": [
    "This generates a full list of missing-value table.\n",
    "\n",
    "We decided to leave as it is. No need to fill ExitDate with NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b3a44-def3-4f99-9bbc-d60169731681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. create a clean copy\n",
    "hr_clean = hr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910be580-d423-4780-947e-dd34be62e84c",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "This will prevent overwriting raw data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66b3c48-5612-4306-94a8-8e46ce25b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Cleaning:\n",
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "|Unnamed: 0|FirstName|LastName|StartDate|ExitDate|Title|Supervisor|ADEmail|BusinessUnit|EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|TerminationDescription|DepartmentType|Division|DOB|State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance Score|Current Employee Rating|Employee ID|Survey Date|Engagement Score|Satisfaction Score|Work-Life Balance Score|Training Date|Training Program Name|Training Type|Training Outcome|Location|Trainer|Training Duration(Days)|Training Cost|\n",
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "|0         |0        |0       |0        |1544    |0    |0         |0      |0           |0             |0           |0      |0                         |0              |0                     |0             |0       |0  |0    |0                     |0         |0           |0       |0          |0                |0                      |0          |0          |0               |0                 |0                      |0            |0                    |0            |0               |0       |0      |0                      |0            |\n",
      "+----------+---------+--------+---------+--------+-----+----------+-------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+--------------+--------+---+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+--------+-------+-----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#c. handling the missing values\n",
    "from pyspark.sql.functions import col\n",
    "# Replace missing termination description with \"Not Applicable\"\n",
    "hr_clean = hr_clean.fillna({\"TerminationDescription\": \"Not Applicable\"})\n",
    "\n",
    "print(\"Missing Values After Cleaning:\")\n",
    "missing_report(hr_clean).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b2a73-ccef-478d-ad1a-a0465623ca70",
   "metadata": {},
   "source": [
    "<p>We identified missing values in ExitDate and TerminationDescription.</p>\n",
    "<p>- Missing ExitDate values indicate that an employee is still active, therefore these nulls were intentionally preserved and later used to compute the TurnoverFlag.</br>\n",
    "- Missing TerminationDescription appears only for active employees because no termination reason exists. These were replaced with \"Not Applicable\" to prevent issues during reporting and segmentation..</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc99f46-7be6-409a-a580-aa9fc5785db3",
   "metadata": {},
   "source": [
    "<h3>Step 4: Perform the Data Transformation for HR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57f170e-2edc-4253-b178-1b1d23d905f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning column names:\n",
      "root\n",
      " |-- Unnamed:_0: integer (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- StartDate: string (nullable = true)\n",
      " |-- ExitDate: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Supervisor: string (nullable = true)\n",
      " |-- ADEmail: string (nullable = true)\n",
      " |-- BusinessUnit: string (nullable = true)\n",
      " |-- EmployeeStatus: string (nullable = true)\n",
      " |-- EmployeeType: string (nullable = true)\n",
      " |-- PayZone: string (nullable = true)\n",
      " |-- EmployeeClassificationType: string (nullable = true)\n",
      " |-- TerminationType: string (nullable = true)\n",
      " |-- TerminationDescription: string (nullable = false)\n",
      " |-- DepartmentType: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- JobFunctionDescription: string (nullable = true)\n",
      " |-- GenderCode: string (nullable = true)\n",
      " |-- LocationCode: integer (nullable = true)\n",
      " |-- RaceDesc: string (nullable = true)\n",
      " |-- MaritalDesc: string (nullable = true)\n",
      " |-- Performance_Score: string (nullable = true)\n",
      " |-- Current_Employee_Rating: integer (nullable = true)\n",
      " |-- Employee_ID: integer (nullable = true)\n",
      " |-- Survey_Date: string (nullable = true)\n",
      " |-- Engagement_Score: integer (nullable = true)\n",
      " |-- Satisfaction_Score: integer (nullable = true)\n",
      " |-- Work_Life_Balance_Score: integer (nullable = true)\n",
      " |-- Training_Date: string (nullable = true)\n",
      " |-- Training_Program_Name: string (nullable = true)\n",
      " |-- Training_Type: string (nullable = true)\n",
      " |-- Training_Outcome: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Trainer: string (nullable = true)\n",
      " |-- Training_DurationDays: integer (nullable = true)\n",
      " |-- Training_Cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#a. clean column names by removing spaces and special characters\n",
    "import re\n",
    "def clean_column_name(col):\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    col = col.replace(\"-\", \"_\")\n",
    "    col = re.sub(r\"[()]\", \"\", col)\n",
    "    return col\n",
    "for col_name in hr_clean.columns:\n",
    "    hr_clean = hr_clean.withColumnRenamed(col_name, clean_column_name(col_name))\n",
    "print(\"After cleaning column names:\")\n",
    "hr_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff0e23-4fa3-4409-b0aa-68d80dd2a28a",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "To improve pipeline transformations and avoid Spark conflicts when referencing column name, all column names were normalized into snake_case and cleaned of special characters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8480888f-a1d2-4d29-a6ac-b7b192147f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting date columns:\n",
      "+----------+--------+----------+-------------+-----------+\n",
      "| StartDate|ExitDate|       DOB|Training_Date|Survey_Date|\n",
      "+----------+--------+----------+-------------+-----------+\n",
      "|2019-09-20|    NULL|1969-10-07|   2023-07-15| 2023-01-14|\n",
      "|2023-02-11|    NULL|1965-08-30|   2022-09-12| 2022-09-09|\n",
      "|2018-12-10|    NULL|1991-10-06|   2022-08-13| 2023-05-27|\n",
      "|2021-06-21|    NULL|1998-04-04|   2022-12-15| 2023-06-16|\n",
      "|2019-06-29|    NULL|1969-08-29|   2023-07-13| 2022-11-25|\n",
      "+----------+--------+----------+-------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#b. convert date columns to correct date fields\n",
    "from pyspark.sql.functions import to_date, col\n",
    "date_formats = {\n",
    "    \"StartDate\": \"dd-MMM-yy\",\n",
    "    \"ExitDate\": \"dd-MMM-yy\",\n",
    "    \"DOB\": \"dd-MM-yyyy\",\n",
    "    \"Training_Date\": \"dd-MMM-yy\",\n",
    "    \"Survey_Date\": \"dd-MM-yyyy\"\n",
    "}\n",
    "for c, fmt in date_formats.items():\n",
    "    if c in hr_clean.columns:\n",
    "        hr_clean = hr_clean.withColumn(c, to_date(col(c), fmt))\n",
    "print(\"After converting date columns:\")\n",
    "hr_clean.select(\"StartDate\", \"ExitDate\", \"DOB\", \"Training_Date\", \"Survey_Date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859efaa8-7135-4b9b-b6fd-a5d3b0e58a1d",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "Date fields were converted from string to date types to allow calculations for tenure, age, training timelines and other time-based analytics</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0a98b9-476d-4e39-9767-029f88ccc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age column sample:\n",
      "+----------+---+\n",
      "|       DOB|Age|\n",
      "+----------+---+\n",
      "|1969-10-07| 56|\n",
      "|1965-08-30| 60|\n",
      "|1991-10-06| 34|\n",
      "|1998-04-04| 27|\n",
      "|1969-08-29| 56|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#c. create new column for age\n",
    "from pyspark.sql.functions import months_between, current_date, floor\n",
    "hr_clean = hr_clean.withColumn(\"Age\", floor(months_between(current_date(), col(\"DOB\")) / 12))\n",
    "print(\"Age column sample:\")\n",
    "hr_clean.select(\"DOB\", \"Age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de68b918-82a4-42e2-8524-81c14c7d3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|AgeGroup|count|\n",
      "+--------+-----+\n",
      "|   30-39|  550|\n",
      "|     60+| 1315|\n",
      "|   40-49|  512|\n",
      "|Under 30|  316|\n",
      "|   50-59|  457|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#d. create new column for agegroup\n",
    "from pyspark.sql.functions import when, col\n",
    "hr_clean = hr_clean.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Age\") < 30, \"Under 30\")\n",
    "    .when((col(\"Age\") >= 30) & (col(\"Age\") <= 39), \"30-39\")\n",
    "    .when((col(\"Age\") >= 40) & (col(\"Age\") <= 49), \"40-49\")\n",
    "    .when((col(\"Age\") >= 50) & (col(\"Age\") <= 59), \"50-59\")\n",
    "    .otherwise(\"60+\")\n",
    ")\n",
    "hr_clean.groupBy(\"AgeGroup\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cdbc1-b0c5-47b6-9011-374d0ca57071",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "- DOB is not useful for analytics, however we can calculate the age of employees<br>\n",
    "- AgeGroup is required for workforce segmentation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6422d65b-9c91-4472-b023-d8da4e3021b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenure column sample:\n",
      "+----------+------+\n",
      "| StartDate|Tenure|\n",
      "+----------+------+\n",
      "|2019-09-20|     6|\n",
      "|2023-02-11|     2|\n",
      "|2018-12-10|     6|\n",
      "|2021-06-21|     4|\n",
      "|2019-06-29|     6|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#e. create new column for tenure(in years)\n",
    "hr_clean = hr_clean.withColumn(\"Tenure\", floor(months_between(current_date(), col(\"StartDate\")) / 12))\n",
    "print(\"Tenure column sample:\")\n",
    "hr_clean.select(\"StartDate\", \"Tenure\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937b6b2-860c-4dfb-8aaa-88b314220ee8",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "- The StartDate alone isn’t actionable. We have created a tenure column which is needed for workforce analysis<br>\n",
    "- Helps evaluate employee stability and retention</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a913c001-766c-4a24-8824-fe225ad68e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turnover flag sample:\n",
      "+--------+------------+\n",
      "|ExitDate|TurnoverFlag|\n",
      "+--------+------------+\n",
      "|    NULL|           0|\n",
      "|    NULL|           0|\n",
      "|    NULL|           0|\n",
      "|    NULL|           0|\n",
      "|    NULL|           0|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#f. turnover flag for active or left employees\n",
    "from pyspark.sql.functions import when, col\n",
    "hr_clean = hr_clean.withColumn(\n",
    "    \"TurnoverFlag\",\n",
    "    when(col(\"ExitDate\").isNull(), 0).otherwise(1)\n",
    ")\n",
    "print(\"Turnover flag sample:\")\n",
    "hr_clean.select(\"ExitDate\", \"TurnoverFlag\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732104a5-b720-4e31-ac25-134db93e9cf2",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "- The Turnover is currently 0 for active employees and 1 for left employees.<br>\n",
    "- Required for turnover analysis by department, gender, race, etc</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df905a5d-ee77-4b27-851f-52fec345ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g. convert numeric columns\n",
    "numeric_cols = [\n",
    "    \"Training_DurationDays\",\n",
    "    \"Training_Cost\",\n",
    "    \"Engagement_Score\",\n",
    "    \"Satisfaction_Score\",\n",
    "    \"Work_Life_Balance_Score\",\n",
    "    \"Current_Employee_Rating\"\n",
    "]\n",
    "for c in numeric_cols:\n",
    "    if c in hr_clean.columns:\n",
    "        hr_clean = hr_clean.withColumn(c, col(c).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9750e4-08f8-4177-930f-1235da045233",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "- Numeric attributes were cast to double precision to support analytic operations, aggregations and model-friendly formats<br>\n",
    "- Required for calculations, averages, correlations and ML</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4091da2-eeb3-40fd-b580-f8369f064d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h. drop useless columns\n",
    "cols_to_drop = [\"Unnamed:_0\", \"ADEmail\", \"Supervisor\", \"Trainer\"]\n",
    "hr_clean = hr_clean.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5700c6c-144d-486f-be6f-b0e60a66dd72",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "We are deleting these columns because they do not contribute to the analytical goals of the project. Below is a breakdown:<br>\n",
    "- Unnamed:_0 is an index generated during CSV export and has no semantic meaning<br>\n",
    "- ADEmail contains personal identifiers that do not support analysis and should not be included for privacy reasons<br>\n",
    "- Supervisor and Trainer contain high-cardinality values that are not required for the KPIs or transformations in this pipeline and retaining them introduces unnecessary noise</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e012b78-5c90-4c80-a79f-c322c521de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned data for HR data:\n",
      "+---------+--------+----------+--------+-----------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+----------------------+-------------+----------------+----------------+---------------------+-------------+---+--------+------+------------+\n",
      "|FirstName|LastName|StartDate |ExitDate|Title                  |BusinessUnit|EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|TerminationDescription|DepartmentType   |Division            |DOB       |State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance_Score|Current_Employee_Rating|Employee_ID|Survey_Date|Engagement_Score|Satisfaction_Score|Work_Life_Balance_Score|Training_Date|Training_Program_Name |Training_Type|Training_Outcome|Location        |Training_DurationDays|Training_Cost|Age|AgeGroup|Tenure|TurnoverFlag|\n",
      "+---------+--------+----------+--------+-----------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+----------------------+-------------+----------------+----------------+---------------------+-------------+---+--------+------+------------+\n",
      "|Uriah    |Bridges |2019-09-20|NULL    |Production Technician I|CCDR        |Active        |Contract    |Zone C |Temporary                 |Unk            |Not Applicable        |Production       |Finance & Accounting|1969-10-07|MA   |Accounting            |Female    |34904       |White   |Widowed    |Fully Meets      |4.0                    |3427       |2023-01-14 |1.0             |2.0               |3.0                    |2023-07-15   |Leadership Development|Internal     |Failed          |South Marisa    |2.0                  |606.11       |56 |50-59   |6     |0           |\n",
      "|Paula    |Small   |2023-02-11|NULL    |Production Technician I|EW          |Active        |Contract    |Zone A |Part-Time                 |Unk            |Not Applicable        |Production       |Aerial              |1965-08-30|MA   |Labor                 |Male      |6593        |Hispanic|Widowed    |Fully Meets      |3.0                    |3428       |2022-09-09 |2.0             |1.0               |5.0                    |2022-09-12   |Customer Service      |External     |Incomplete      |Tammieville     |4.0                  |673.02       |60 |60+     |2     |0           |\n",
      "|Edward   |Buck    |2018-12-10|NULL    |Area Sales Manager     |PL          |Active        |Full-Time   |Zone B |Part-Time                 |Unk            |Not Applicable        |Sales            |General - Sga       |1991-10-06|MA   |Assistant             |Male      |2330        |Hispanic|Widowed    |Fully Meets      |4.0                    |3429       |2023-05-27 |1.0             |2.0               |1.0                    |2022-08-13   |Leadership Development|External     |Failed          |East Roberthaven|2.0                  |413.28       |34 |30-39   |6     |0           |\n",
      "|Michael  |Riordan |2021-06-21|NULL    |Area Sales Manager     |CCDR        |Active        |Contract    |Zone A |Full-Time                 |Unk            |Not Applicable        |Sales            |Finance & Accounting|1998-04-04|ND   |Clerk                 |Male      |58782       |Other   |Single     |Fully Meets      |2.0                    |3430       |2023-06-16 |5.0             |5.0               |4.0                    |2022-12-15   |Project Management    |External     |Completed       |Garzatown       |3.0                  |663.78       |27 |Under 30|4     |0           |\n",
      "|Jasmine  |Onque   |2019-06-29|NULL    |Area Sales Manager     |TNS         |Active        |Contract    |Zone A |Temporary                 |Unk            |Not Applicable        |Sales            |General - Con       |1969-08-29|FL   |Laborer               |Female    |33174       |Other   |Married    |Fully Meets      |3.0                    |3431       |2022-11-25 |2.0             |5.0               |3.0                    |2023-07-13   |Technical Skills      |External     |Failed          |Lake Meganville |5.0                  |399.03       |56 |50-59   |6     |0           |\n",
      "+---------+--------+----------+--------+-----------------------+------------+--------------+------------+-------+--------------------------+---------------+----------------------+-----------------+--------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+----------------------+-------------+----------------+----------------+---------------------+-------------+---+--------+------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned data for HR data:\")\n",
    "hr_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3914afc-f6e5-4884-adfb-1b0ed6467243",
   "metadata": {},
   "source": [
    "<h3>Step 6: Import Datasets for HR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad43114-4755-41b8-8748-4438b900d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------+------+-------------------------+---------+-----------+----------------------+---------------+-------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "|Hash1                                   |Hash2                                   |Birth Year|Gender|Race                     |Pay Grade|FLSA Status|EEOJ Code             |Employment Type|Department ID|Department Name       |Status   |Year|Date Position Began|Date Position Ended|Annual Base Salary|Gross Pay|Base Pay|Overtime Pay|Other Pay|\n",
      "+----------------------------------------+----------------------------------------+----------+------+-------------------------+---------+-----------+----------------------+---------------+-------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "|C99E0C04311A1FBD86A68E01527B3D4F9CFAF656|8FBD6E88FE846F798156B53582A07FE68C24D5D8|1955      |Female|Black or African American|15       |Non-Exempt |B: Professionals      |Full Time      |1600         |Municipal Courts      |Withdrawn|2025|01/01/2025         |01/02/2025         |51667.0           |1286.61  |1192.32 |0.0         |94.29    |\n",
      "|F224E67D62558473D8FD5F0332489905C592CDB5|4F2E3C274FFDC92F8436C9E15BF42DEDA1B62DEA|1955      |Female|White                    |20       |Exempt     |B: Professionals      |Full Time      |3600         |Parks & Recreation    |Withdrawn|2025|01/01/2025         |01/02/2025         |67337.0           |1890.61  |1812.91 |0.0         |77.7     |\n",
      "|097646E986686B036D654E005F9581BE32377675|D9EFEE57EBA8157AAEBE1FC9B3EAEBD0348C7652|1963      |Male  |Black or African American|27       |Exempt     |B: Professionals      |Full Time      |1000         |Police                |Withdrawn|2025|01/01/2025         |01/02/2025         |117620.0          |98009.09 |3166.68 |0.0         |94842.41 |\n",
      "|2010C72D5CEE0AE8FA62F194DCEA2C1BAF1211A0|A4B01138B5CF79624949F94BDAE9366E6469F5E8|1973      |Male  |Black or African American|06       |Non-Exempt |H: Service/Maintenance|Full Time      |2000         |Houston Public Works  |Withdrawn|2025|01/01/2025         |01/02/2025         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |\n",
      "|51A39920542A5C29D48B4EE8955CAEB3F7FF5322|50FAA8C948DD01067D238331EABD3E888DFE1033|1959      |Male  |Black or African American|13       |Non-Exempt |H: Service/Maintenance|Full Time      |2100         |Solid Waste Management|Withdrawn|2025|01/01/2025         |01/02/2025         |58386.0           |20728.21 |1571.92 |0.0         |19156.29 |\n",
      "+----------------------------------------+----------------------------------------+----------+------+-------------------------+---------+-----------+----------------------+---------------+-------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "pay_df = spark.read.csv(\"payroll-2025.csv\", header=True, inferSchema=True)\n",
    "pay_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcad0-1705-4699-8e8c-2035638a0aef",
   "metadata": {},
   "source": [
    "<h3>Step 7: Perform Cleaning for Payroll dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf03533-9c38-4516-a30f-9b7523c05648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. create clean copy\n",
    "pay_clean = pay_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2422180-743a-42b9-a47e-acd92f30e4aa",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "This is to keep pay_df, which is the original dataset and avoid overwriting it during cleaning</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e95ed26-1ba7-47d2-afda-5fc4baf5972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "|Hash1|Hash2|Birth Year|Gender|Race|Pay Grade|FLSA Status|EEOJ Code|Employment Type|Department ID|Department Name|Status|Year|Date Position Began|Date Position Ended|Annual Base Salary|Gross Pay|Base Pay|Overtime Pay|Other Pay|\n",
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "|    0|    0|         0|     0|   0|        0|          0|        0|              0|            0|              0|     0|   0|                  0|              22462|                 0|        0|       0|           0|        0|\n",
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#b. view missing values for payroll\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "# Count nulls per column\n",
    "def missing_report(df):\n",
    "    return df.select([\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "missing_report(pay_clean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "249c7b9a-ff1c-4cf4-9776-481d809eeaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+\n",
      "|Hash1|Hash2|Birth Year|Gender|Race|Pay Grade|FLSA Status|EEOJ Code|Employment Type|Department ID|Department Name|Status|Year|Date Position Began|Date Position Ended|Annual Base Salary|Gross Pay|Base Pay|Overtime Pay|Other Pay|PositionEndFlag|\n",
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+\n",
      "|0    |0    |0         |0     |0   |0        |0          |0        |0              |0            |0              |0     |0   |0                  |22462              |0                 |0        |0       |0           |0        |0              |\n",
      "+-----+-----+----------+------+----+---------+-----------+---------+---------------+-------------+---------------+------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"PositionEndFlag\",\n",
    "    when(col(\"Date Position Ended\").isNull(), \"Not Ended\").otherwise(\"Ended\")\n",
    ")\n",
    "missing_report(pay_clean).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331de60b-688f-4611-8044-111f63dd5b24",
   "metadata": {},
   "source": [
    "<p>As you can see, there is Date Position Ended column with missing values. These null values are intentional because they represent employees whose job positions were still active at the time of data extraction.</br>\n",
    "Since this information is meaningful, the null values were preserved and used to derive an additional flag to support retention and workforce stability analysis.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb22e1-d927-4ee7-a403-03b30e213083",
   "metadata": {},
   "source": [
    "<h3>Step 8: Perform the Data Transformation for Payroll Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687e0586-7b48-4899-9b35-057bb56fecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning column names:\n",
      "root\n",
      " |-- Hash1: string (nullable = true)\n",
      " |-- Hash2: string (nullable = true)\n",
      " |-- Birth_Year: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Pay_Grade: string (nullable = true)\n",
      " |-- FLSA_Status: string (nullable = true)\n",
      " |-- EEOJ_Code: string (nullable = true)\n",
      " |-- Employment_Type: string (nullable = true)\n",
      " |-- Department_ID: integer (nullable = true)\n",
      " |-- Department_Name: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Date_Position_Began: string (nullable = true)\n",
      " |-- Date_Position_Ended: string (nullable = true)\n",
      " |-- Annual_Base_Salary: double (nullable = true)\n",
      " |-- Gross_Pay: double (nullable = true)\n",
      " |-- Base_Pay: double (nullable = true)\n",
      " |-- Overtime_Pay: double (nullable = true)\n",
      " |-- Other_Pay: double (nullable = true)\n",
      " |-- PositionEndFlag: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#a. clean the column names \n",
    "import re\n",
    "def clean_column_name(col):\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    col = col.replace(\"-\", \"_\")\n",
    "    col = re.sub(r\"[()]\", \"\", col)\n",
    "    return col\n",
    "for col_name in pay_clean.columns:\n",
    "    pay_clean = pay_clean.withColumnRenamed(col_name, clean_column_name(col_name))\n",
    "print(\"After cleaning column names:\")\n",
    "pay_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d5acd-392c-412c-9f71-427e4972f962",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "To avoid Spark errors and ensure consistent naming for transformations and joins</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4331564e-2c76-4072-bbd5-c3b4307634bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting date columns:\n",
      "+-------------------+-------------------+\n",
      "|Date_Position_Began|Date_Position_Ended|\n",
      "+-------------------+-------------------+\n",
      "|         2025-01-01|         2025-02-01|\n",
      "|         2025-01-01|         2025-02-01|\n",
      "|         2025-01-01|         2025-02-01|\n",
      "|         2025-01-01|         2025-02-01|\n",
      "|         2025-01-01|         2025-02-01|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#b. convert date fields\n",
    "from pyspark.sql.functions import to_date, col\n",
    "date_formats = {\n",
    "    \"Date_Position_Began\": \"dd/MM/yyyy\",\n",
    "    \"Date_Position_Ended\": \"dd/MM/yyyy\"\n",
    "}\n",
    "for c, fmt in date_formats.items():\n",
    "    if c in pay_clean.columns:\n",
    "        pay_clean = pay_clean.withColumn(c, to_date(col(c), fmt))\n",
    "print(\"After converting date columns:\")\n",
    "pay_clean.select(\"Date_Position_Began\", \"Date_Position_Ended\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e45776-8a41-4a1a-843a-fff72d6bd4dc",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "Convert date strings into valid date types to calculate tenure and length of service</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eb7d322-235b-474a-993a-23a753f64609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning the race column:\n",
      "+--------+\n",
      "|Race    |\n",
      "+--------+\n",
      "|Black   |\n",
      "|White   |\n",
      "|Black   |\n",
      "|Black   |\n",
      "|Black   |\n",
      "|Asian   |\n",
      "|Black   |\n",
      "|Black   |\n",
      "|Hispanic|\n",
      "|White   |\n",
      "+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#c. clean the race column\n",
    "from pyspark.sql.functions import when, col\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"Race\",\n",
    "    when(col(\"Race\") == \"Asian\", \"Asian\")\n",
    "    .when(col(\"Race\") == \"Black or African American\", \"Black\")\n",
    "    .when(col(\"Race\") == \"Hispanic/Latino\", \"Hispanic\")\n",
    "    .when(col(\"Race\") == \"White\", \"White\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "print(\"After cleaning the race column:\")\n",
    "pay_clean.select(\"Race\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d40835-64f4-448c-a40d-8dfc4f46be7b",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "Race column was standardised into simplified, analysis-friendly categories. This is necessary because the original dataset included long and inconsistent labels such as “Black or African American” and “Hispanic/Latino”. A new field Race_Clean was created to group race values into consistent categories including “Black”, “Hispanic”, “Asian”, “White”, and “Other”.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8820a6c-bf27-4920-91ef-3c7a5e7cd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. convert numeric columns\n",
    "numeric_cols = [\n",
    "    \"Birth_Year\",\n",
    "    \"Pay_Grade\",\n",
    "    \"Annual_Base_Salary\",\n",
    "    \"Gross_Pay\",\n",
    "    \"Base_Pay\",\n",
    "    \"Overtime_Pay\",\n",
    "    \"Other_Pay\"\n",
    "]\n",
    "for c in numeric_cols:\n",
    "    if c in pay_clean.columns:\n",
    "        pay_clean = pay_clean.withColumn(c, col(c).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5bd551-2633-4539-94b5-082014214ed5",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "These columns must be numeric for aggregations, analytics and creating KPIs like total compensation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef70ba5-0519-40f4-9d2e-942292895b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years of Service sample:\n",
      "+-------------------+--------------+\n",
      "|Date_Position_Began|YearsOfService|\n",
      "+-------------------+--------------+\n",
      "|         2025-01-01|             0|\n",
      "|         2025-01-01|             0|\n",
      "|         2025-01-01|             0|\n",
      "|         2025-01-01|             0|\n",
      "|         2025-01-01|             0|\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#e. create a new column for years of service\n",
    "from pyspark.sql.functions import current_date, months_between, floor\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"YearsOfService\",\n",
    "    floor(months_between(current_date(), col(\"Date_Position_Began\")) / 12)\n",
    ")\n",
    "print(\"Years of Service sample:\")\n",
    "pay_clean.select(\"Date_Position_Began\", \"YearsOfService\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344a660-7407-4fa6-8baf-2d90892b3648",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "This supports workforce finance analytics such as salary growth vs. years of service</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "237d1912-db67-4d9b-a54a-38e1c3984b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compensation sample:\n",
      "+--------+------------+---------+------------------+\n",
      "|Base_Pay|Overtime_Pay|Other_Pay| TotalCompensation|\n",
      "+--------+------------+---------+------------------+\n",
      "| 1192.32|         0.0|    94.29|           1286.61|\n",
      "| 1812.91|         0.0|     77.7|1890.6100000000001|\n",
      "| 3166.68|         0.0| 94842.41|          98009.09|\n",
      "|  1090.2|       290.3|   797.88|           2178.38|\n",
      "| 1571.92|         0.0| 19156.29|          20728.21|\n",
      "+--------+------------+---------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#f. create a new column for total compensation\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"TotalCompensation\",\n",
    "    col(\"Base_Pay\") + col(\"Overtime_Pay\") + col(\"Other_Pay\")\n",
    ")\n",
    "print(\"Total compensation sample:\")\n",
    "pay_clean.select(\"Base_Pay\", \"Overtime_Pay\", \"Other_Pay\", \"TotalCompensation\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5fe7c-be6e-42b9-83f8-c292945a4e09",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "This gives a realistic financial metric of earnings and not just base salary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30260ca3-435e-4d33-9012-65b30f814776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|Pay_Grade|PayGradeCategory|\n",
      "+---------+----------------+\n",
      "|     15.0|             Mid|\n",
      "|     20.0|             Mid|\n",
      "|     27.0|            High|\n",
      "|      6.0|             Low|\n",
      "|     13.0|             Mid|\n",
      "+---------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#g. create a new column for pay grade category\n",
    "from pyspark.sql.functions import when\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"PayGradeCategory\",\n",
    "    when(col(\"Pay_Grade\") <= 10, \"Low\")\n",
    "    .when((col(\"Pay_Grade\") > 10) & (col(\"Pay_Grade\") <= 20), \"Mid\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "pay_clean.select(\"Pay_Grade\", \"PayGradeCategory\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b6b73-9bc9-4c30-9157-3ea4073387f1",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "Payment grade helps compare compensation across departments and demographics</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b48c08e5-ffef-4f74-a8d5-eb30843a5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment active status sample:\n",
      "+---------+--------+\n",
      "|   Status|IsActive|\n",
      "+---------+--------+\n",
      "|Withdrawn|       0|\n",
      "|Withdrawn|       0|\n",
      "|Withdrawn|       0|\n",
      "|Withdrawn|       0|\n",
      "|Withdrawn|       0|\n",
      "+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#h. create a new column for employment status flag\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"IsActive\",\n",
    "    when(col(\"Status\") == \"Active\", 1).otherwise(0)\n",
    ")\n",
    "print(\"Employment active status sample:\")\n",
    "pay_clean.select(\"Status\", \"IsActive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952fbb7b-808c-437a-add3-2d2d1006b4a9",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "Required for turnover and workforce stability analytics</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f641a77-27ce-495a-ba9e-61d6c3cf9863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|Birth_Year| Age|\n",
      "+----------+----+\n",
      "|    1955.0|70.0|\n",
      "|    1955.0|70.0|\n",
      "|    1963.0|62.0|\n",
      "|    1973.0|52.0|\n",
      "|    1959.0|66.0|\n",
      "|    1958.0|67.0|\n",
      "|    1959.0|66.0|\n",
      "|    1948.0|77.0|\n",
      "|    1960.0|65.0|\n",
      "|    1954.0|71.0|\n",
      "+----------+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#i. create Age in Payroll dataset\n",
    "from pyspark.sql.functions import col\n",
    "current_year = 2025\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"Age\",\n",
    "    current_year - col(\"Birth_Year\")\n",
    ")\n",
    "pay_clean.select(\"Birth_Year\", \"Age\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a538b636-4b63-428b-9de5-52d1ddf28825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|AgeGroup|count|\n",
      "+--------+-----+\n",
      "|   30-39| 6401|\n",
      "|     60+| 3569|\n",
      "|   40-49| 6437|\n",
      "|Under 30| 4572|\n",
      "|   50-59| 6230|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#j. create AgeGroup\n",
    "from pyspark.sql.functions import when\n",
    "pay_clean = pay_clean.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Age\") < 30, \"Under 30\")\n",
    "    .when((col(\"Age\") >= 30) & (col(\"Age\") <= 39), \"30-39\")\n",
    "    .when((col(\"Age\") >= 40) & (col(\"Age\") <= 49), \"40-49\")\n",
    "    .when((col(\"Age\") >= 50) & (col(\"Age\") <= 59), \"50-59\")\n",
    "    .otherwise(\"60+\")\n",
    ")\n",
    "pay_clean.groupBy(\"AgeGroup\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896f8edc-7eae-4f90-aa85-f3ad9c145dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k. drop useless columns\n",
    "cols_to_drop = [\"Hash1\", \"Hash2\", \"Department_ID\"]\n",
    "pay_clean = pay_clean.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb50edc-9242-456a-a4c8-28bd80965b31",
   "metadata": {},
   "source": [
    "<p>Reason:<br>\n",
    "- Hash1 and Hash2 are hashed IDs and do not match the HR dataset</br>\n",
    "- Department ID are internal codes used only inside the Houston payroll system</br>\n",
    "- They cannot be used for merging</br>\n",
    "- They contribute no analytical value</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1231588a-6378-43b0-bc17-df8291dfdc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned payroll data:\n",
      "+----------+------+-----+---------+-----------+----------------------+---------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+------------------+----------------+--------+----+--------+\n",
      "|Birth_Year|Gender|Race |Pay_Grade|FLSA_Status|EEOJ_Code             |Employment_Type|Department_Name       |Status   |Year|Date_Position_Began|Date_Position_Ended|Annual_Base_Salary|Gross_Pay|Base_Pay|Overtime_Pay|Other_Pay|PositionEndFlag|YearsOfService|TotalCompensation |PayGradeCategory|IsActive|Age |AgeGroup|\n",
      "+----------+------+-----+---------+-----------+----------------------+---------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+------------------+----------------+--------+----+--------+\n",
      "|1955.0    |Female|Black|15.0     |Non-Exempt |B: Professionals      |Full Time      |Municipal Courts      |Withdrawn|2025|2025-01-01         |2025-02-01         |51667.0           |1286.61  |1192.32 |0.0         |94.29    |Ended          |0             |1286.61           |Mid             |0       |70.0|60+     |\n",
      "|1955.0    |Female|White|20.0     |Exempt     |B: Professionals      |Full Time      |Parks & Recreation    |Withdrawn|2025|2025-01-01         |2025-02-01         |67337.0           |1890.61  |1812.91 |0.0         |77.7     |Ended          |0             |1890.6100000000001|Mid             |0       |70.0|60+     |\n",
      "|1963.0    |Male  |Black|27.0     |Exempt     |B: Professionals      |Full Time      |Police                |Withdrawn|2025|2025-01-01         |2025-02-01         |117620.0          |98009.09 |3166.68 |0.0         |94842.41 |Ended          |0             |98009.09          |High            |0       |62.0|60+     |\n",
      "|1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works  |Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38           |Low             |0       |52.0|50-59   |\n",
      "|1959.0    |Male  |Black|13.0     |Non-Exempt |H: Service/Maintenance|Full Time      |Solid Waste Management|Withdrawn|2025|2025-01-01         |2025-02-01         |58386.0           |20728.21 |1571.92 |0.0         |19156.29 |Ended          |0             |20728.21          |Mid             |0       |66.0|60+     |\n",
      "+----------+------+-----+---------+-----------+----------------------+---------------+----------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+------------------+----------------+--------+----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned payroll data:\")\n",
    "pay_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa703202-793d-4447-b6bc-f3eba30b10d5",
   "metadata": {},
   "source": [
    "<h3>Step 9: Perform Data Transformation (Cont'd) for Departments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7734fe36-fa15-4337-928f-b2432e36f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|DepartmentType      |\n",
      "+--------------------+\n",
      "|Sales               |\n",
      "|Production          |\n",
      "|Admin Offices       |\n",
      "|Executive Office    |\n",
      "|Software Engineering|\n",
      "|IT/IS               |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hr_clean.select(\"DepartmentType\").distinct().show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fdcd58d-ce6e-436f-934d-4deafe436676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|Department_Name              |\n",
      "+-----------------------------+\n",
      "|Houston Emergency Center     |\n",
      "|Library                      |\n",
      "|Municipal Courts             |\n",
      "|Health & Human Services      |\n",
      "|Houston Airport System       |\n",
      "|Information Technology       |\n",
      "|Finance                      |\n",
      "|Mayor's Office               |\n",
      "|Solid Waste Management       |\n",
      "|Houston Public Works         |\n",
      "|Business Opportunity         |\n",
      "|Police                       |\n",
      "|Parks & Recreation           |\n",
      "|Fleet Management             |\n",
      "|Legal                        |\n",
      "|General Services             |\n",
      "|Admin. & Regulatory Affairs  |\n",
      "|City Secretary               |\n",
      "|Housing & Community Devlpmnt.|\n",
      "|City Council                 |\n",
      "|Fire                         |\n",
      "|Planning & Development       |\n",
      "|Department of Neighborhoods  |\n",
      "|Controllers                  |\n",
      "|Human Resources              |\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pay_clean.select(\"Department_Name\").distinct().show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fd0218a-39ed-44ee-a03c-a76754a9a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|      DepartmentType|DeptCategory|count|\n",
      "+--------------------+------------+-----+\n",
      "|Software Engineering|  Technology|  121|\n",
      "|       Admin Offices|   Corporate|   85|\n",
      "|               IT/IS|  Technology|  459|\n",
      "|               Sales|  Operations|  345|\n",
      "|    Executive Office|   Corporate|   25|\n",
      "|   Production       |       Other| 2115|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the mapping table for HR Department Type\n",
    "from pyspark.sql.functions import when, col\n",
    "hr_with_cat = hr_clean.withColumn(\n",
    "    \"DeptCategory\",\n",
    "    when(col(\"DepartmentType\") == \"Sales\", \"Operations\")\n",
    "    .when(col(\"DepartmentType\") == \"Production\", \"Operations\")\n",
    "    .when(col(\"DepartmentType\") == \"Admin Offices\", \"Corporate\")\n",
    "    .when(col(\"DepartmentType\") == \"Executive Office\", \"Corporate\")\n",
    "    .when(col(\"DepartmentType\") == \"Software Engineering\", \"Technology\")\n",
    "    .when(col(\"DepartmentType\") == \"IT/IS\", \"Technology\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "# Optional: quick sanity check\n",
    "hr_with_cat.groupBy(\"DepartmentType\", \"DeptCategory\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "322e416f-a9a5-4bb7-940f-449b44395266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-----+\n",
      "|     Department_Name|  DeptCategory|count|\n",
      "+--------------------+--------------+-----+\n",
      "|Houston Emergency...|Public Service|  268|\n",
      "|Admin. & Regulato...|     Corporate|  550|\n",
      "|  Parks & Recreation|Public Service| 1150|\n",
      "|Solid Waste Manag...|    Operations|  503|\n",
      "|        City Council|Public Service|  124|\n",
      "|Planning & Develo...|Public Service|   99|\n",
      "|Houston Public Works|    Operations| 4462|\n",
      "|         Controllers|     Corporate|   82|\n",
      "|             Finance|     Corporate|  294|\n",
      "|    Municipal Courts|Public Service|  292|\n",
      "|      Mayor's Office|     Corporate|  124|\n",
      "|    Fleet Management|    Operations|  410|\n",
      "|Health & Human Se...|Public Service| 1325|\n",
      "|Houston Airport S...|    Operations| 1631|\n",
      "|Information Techn...|    Technology|  311|\n",
      "|    General Services|    Operations|  273|\n",
      "|Business Opportunity|    Operations|   46|\n",
      "|      City Secretary|     Corporate|   11|\n",
      "|     Human Resources|     Corporate|  735|\n",
      "|             Library|Public Service|  591|\n",
      "+--------------------+--------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Create the mapping table for Payroll Department\n",
    "pay_with_cat = pay_clean.withColumn(\n",
    "    \"DeptCategory\",\n",
    "    when(col(\"Department_Name\") == \"Information Technology\", \"Technology\")\n",
    "    # Corporate\n",
    "    .when(col(\"Department_Name\").isin(\n",
    "        \"Finance\",\n",
    "        \"Mayor's Office\",\n",
    "        \"Admin. & Regulatory Affairs\",\n",
    "        \"City Secretary\",\n",
    "        \"Controllers\",\n",
    "        \"Human Resources\",\n",
    "        \"Legal\"\n",
    "    ), \"Corporate\")\n",
    "    # Operations\n",
    "    .when(col(\"Department_Name\").isin(\n",
    "        \"Fleet Management\",\n",
    "        \"General Services\",\n",
    "        \"Solid Waste Management\",\n",
    "        \"Houston Public Works\",\n",
    "        \"Houston Airport System\",\n",
    "        \"Business Opportunity\"\n",
    "    ), \"Operations\")\n",
    "    # Public service\n",
    "    .when(col(\"Department_Name\").isin(\n",
    "        \"Houston Emergency Center\",\n",
    "        \"Library\",\n",
    "        \"Municipal Courts\",\n",
    "        \"Health & Human Services\",\n",
    "        \"Parks & Recreation\",\n",
    "        \"Police\",\n",
    "        \"Fire\",\n",
    "        \"Planning & Development\",\n",
    "        \"Department of Neighborhoods\",\n",
    "        \"Housing & Community Devlpmnt.\",\n",
    "        \"City Council\"\n",
    "    ), \"Public Service\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "# Optional: sanity check\n",
    "pay_with_cat.groupBy(\"Department_Name\", \"DeptCategory\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20e243ed-9e3e-4332-936d-7829769f0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Categories:\n",
      "+------------+\n",
      "|DeptCategory|\n",
      "+------------+\n",
      "|       Other|\n",
      "|  Technology|\n",
      "|   Corporate|\n",
      "|  Operations|\n",
      "+------------+\n",
      "\n",
      "Payroll Categories:\n",
      "+--------------+\n",
      "|  DeptCategory|\n",
      "+--------------+\n",
      "|Public Service|\n",
      "|    Technology|\n",
      "|     Corporate|\n",
      "|    Operations|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"HR Categories:\")\n",
    "hr_with_cat.select(\"DeptCategory\").distinct().show()\n",
    "print(\"Payroll Categories:\")\n",
    "pay_with_cat.select(\"DeptCategory\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06974ef9-06ca-4660-8820-b82f85387c30",
   "metadata": {},
   "source": [
    "<h3>Step 10: Perform Merge</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce50ee7d-5bc6-46e2-9edf-56a3bd9a8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = hr_with_cat.join(\n",
    "    pay_with_cat,\n",
    "    on=\"DeptCategory\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7de626d-dc44-4c42-a867-edfa2cc26260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+----------+----------+------------------------+------------+--------------+------------+-------+--------------------------+---------------+---------------------------------------------+--------------+------------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+-----------------+---------------------+-------------+---+--------+------+------------+----------+------+-----+---------+-----------+----------------------+---------------+--------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+-----------------+----------------+--------+----+--------+\n",
      "|DeptCategory|FirstName|LastName|StartDate |ExitDate  |Title                   |BusinessUnit|EmployeeStatus|EmployeeType|PayZone|EmployeeClassificationType|TerminationType|TerminationDescription                       |DepartmentType|Division                |DOB       |State|JobFunctionDescription|GenderCode|LocationCode|RaceDesc|MaritalDesc|Performance_Score|Current_Employee_Rating|Employee_ID|Survey_Date|Engagement_Score|Satisfaction_Score|Work_Life_Balance_Score|Training_Date|Training_Program_Name|Training_Type|Training_Outcome|Location         |Training_DurationDays|Training_Cost|Age|AgeGroup|Tenure|TurnoverFlag|Birth_Year|Gender|Race |Pay_Grade|FLSA_Status|EEOJ_Code             |Employment_Type|Department_Name     |Status   |Year|Date_Position_Began|Date_Position_Ended|Annual_Base_Salary|Gross_Pay|Base_Pay|Overtime_Pay|Other_Pay|PositionEndFlag|YearsOfService|TotalCompensation|PayGradeCategory|IsActive|Age |AgeGroup|\n",
      "+------------+---------+--------+----------+----------+------------------------+------------+--------------+------------+-------+--------------------------+---------------+---------------------------------------------+--------------+------------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+-----------------+---------------------+-------------+---+--------+------+------------+----------+------+-----+---------+-----------+----------------------+---------------+--------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+-----------------+----------------+--------+----+--------+\n",
      "|Operations  |Cruz     |Arellano|2019-09-17|NULL      |Area Sales Manager      |SVG         |Active        |Full-Time   |Zone B |Full-Time                 |Unk            |Not Applicable                               |Sales         |General - Con           |1993-08-08|AZ   |Laborer               |Male      |19215       |Asian   |Single     |Fully Meets      |4.0                    |2591       |2023-07-28 |4.0             |5.0               |5.0                    |2022-10-06   |Customer Service     |Internal     |Completed       |North Michaelbury|2.0                  |745.31       |32 |30-39   |6     |0           |1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works|Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38          |Low             |0       |52.0|50-59   |\n",
      "|Operations  |Fernando |Richard |2023-07-09|2023-07-14|Administrative Assistant|SVG         |Active        |Contract    |Zone C |Temporary                 |Voluntary      |Continue personal federal another.           |Sales         |General - Sga           |1954-03-08|MA   |Manager               |Female    |15228       |Other   |Widowed    |Fully Meets      |4.0                    |2652       |2022-11-11 |1.0             |4.0               |2.0                    |2022-10-23   |Customer Service     |External     |Incomplete      |Lake Michael     |1.0                  |424.44       |71 |60+     |2     |1           |1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works|Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38          |Low             |0       |52.0|50-59   |\n",
      "|Operations  |Anya     |Baldwin |2019-11-29|2020-04-21|Administrative Assistant|SVG         |Active        |Full-Time   |Zone C |Part-Time                 |Involuntary    |Glass effort however relate partner lay what.|Sales         |General - Con           |2001-03-02|WA   |Laborer               |Female    |44158       |Hispanic|Widowed    |Fully Meets      |1.0                    |2601       |2022-10-06 |3.0             |5.0               |5.0                    |2023-06-29   |Communication Skills |External     |Failed          |Marissastad      |5.0                  |944.24       |24 |Under 30|5     |1           |1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works|Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38          |Low             |0       |52.0|50-59   |\n",
      "|Operations  |Caiden   |Munoz   |2020-12-13|2023-01-13|Area Sales Manager      |TNS         |Active        |Full-Time   |Zone C |Temporary                 |Involuntary    |Red some option.                             |Sales         |Project Management - Con|1963-03-01|ND   |Manager               |Female    |18983       |Black   |Divorced   |Fully Meets      |1.0                    |3520       |2022-09-17 |5.0             |1.0               |1.0                    |2022-09-11   |Customer Service     |External     |Incomplete      |North Anthonybury|1.0                  |607.93       |62 |60+     |4     |1           |1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works|Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38          |Low             |0       |52.0|50-59   |\n",
      "|Operations  |Nevaeh   |Soto    |2023-01-15|NULL      |Area Sales Manager      |SVG         |Active        |Contract    |Zone C |Full-Time                 |Unk            |Not Applicable                               |Sales         |Project Management - Con|1982-01-12|TX   |Director              |Male      |74124       |Other   |Married    |Exceeds          |4.0                    |3472       |2023-07-26 |5.0             |5.0               |4.0                    |2023-02-12   |Technical Skills     |External     |Completed       |Port Karatown    |5.0                  |537.24       |43 |40-49   |2     |0           |1973.0    |Male  |Black|6.0      |Non-Exempt |H: Service/Maintenance|Full Time      |Houston Public Works|Withdrawn|2025|2025-01-01         |2025-02-01         |37794.0           |2178.38  |1090.2  |290.3       |797.88   |Ended          |0             |2178.38          |Low             |0       |52.0|50-59   |\n",
      "+------------+---------+--------+----------+----------+------------------------+------------+--------------+------------+-------+--------------------------+---------------+---------------------------------------------+--------------+------------------------+----------+-----+----------------------+----------+------------+--------+-----------+-----------------+-----------------------+-----------+-----------+----------------+------------------+-----------------------+-------------+---------------------+-------------+----------------+-----------------+---------------------+-------------+---+--------+------+------------+----------+------+-----+---------+-----------+----------------------+---------------+--------------------+---------+----+-------------------+-------------------+------------------+---------+--------+------------+---------+---------------+--------------+-----------------+----------------+--------+----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc7b80cf-729d-4a0f-81fe-937c27136c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged row count: 2927395\n"
     ]
    }
   ],
   "source": [
    "print(\"Merged row count:\", final_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e79a72d7-4ad2-4f18-a7db-e88028e97f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeptCategory',\n",
       " 'FirstName',\n",
       " 'LastName',\n",
       " 'StartDate',\n",
       " 'ExitDate',\n",
       " 'Title',\n",
       " 'BusinessUnit',\n",
       " 'EmployeeStatus',\n",
       " 'EmployeeType',\n",
       " 'PayZone',\n",
       " 'EmployeeClassificationType',\n",
       " 'TerminationType',\n",
       " 'TerminationDescription',\n",
       " 'DepartmentType',\n",
       " 'Division',\n",
       " 'DOB',\n",
       " 'State',\n",
       " 'JobFunctionDescription',\n",
       " 'GenderCode',\n",
       " 'LocationCode',\n",
       " 'RaceDesc',\n",
       " 'MaritalDesc',\n",
       " 'Performance_Score',\n",
       " 'Current_Employee_Rating',\n",
       " 'Employee_ID',\n",
       " 'Survey_Date',\n",
       " 'Engagement_Score',\n",
       " 'Satisfaction_Score',\n",
       " 'Work_Life_Balance_Score',\n",
       " 'Training_Date',\n",
       " 'Training_Program_Name',\n",
       " 'Training_Type',\n",
       " 'Training_Outcome',\n",
       " 'Location',\n",
       " 'Training_DurationDays',\n",
       " 'Training_Cost',\n",
       " 'Age',\n",
       " 'AgeGroup',\n",
       " 'Tenure',\n",
       " 'TurnoverFlag',\n",
       " 'Birth_Year',\n",
       " 'Gender',\n",
       " 'Race',\n",
       " 'Pay_Grade',\n",
       " 'FLSA_Status',\n",
       " 'EEOJ_Code',\n",
       " 'Employment_Type',\n",
       " 'Department_Name',\n",
       " 'Status',\n",
       " 'Year',\n",
       " 'Date_Position_Began',\n",
       " 'Date_Position_Ended',\n",
       " 'Annual_Base_Salary',\n",
       " 'Gross_Pay',\n",
       " 'Base_Pay',\n",
       " 'Overtime_Pay',\n",
       " 'Other_Pay',\n",
       " 'PositionEndFlag',\n",
       " 'YearsOfService',\n",
       " 'TotalCompensation',\n",
       " 'PayGradeCategory',\n",
       " 'IsActive',\n",
       " 'Age',\n",
       " 'AgeGroup']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a985838-707d-4b25-a256-055705cab83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(\"Age\", \"AgeGroup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ebe63-f4d4-4043-b41a-4cc80580e90d",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col\n",
    "final_df = final_df.withColumn(\"Age\", 2025 - col(\"Birth_Year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "352c1a4b-ef1f-406c-9868-fe1f0da3d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "final_df = final_df.withColumn(\"Age\", 2025 - col(\"Birth_Year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d63d007-92d0-41ce-a6f3-f983ac263406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "final_df = final_df.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Age\") < 20, \"Under 20\")\n",
    "    .when((col(\"Age\") >= 20) & (col(\"Age\") < 30), \"20-29\")\n",
    "    .when((col(\"Age\") >= 30) & (col(\"Age\") < 40), \"30-39\")\n",
    "    .when((col(\"Age\") >= 40) & (col(\"Age\") < 50), \"40-49\")\n",
    "    .when((col(\"Age\") >= 50) & (col(\"Age\") < 60), \"50-59\")\n",
    "    .otherwise(\"60+\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64dccdaf-f60d-4b30-a0bd-151a86b9cffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "[col for col, count in Counter(final_df.columns).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37638c37-c403-4341-9bce-0d7a3304380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-------------+\n",
      "|AgeGroup|Gender|    Race|EmployeeCount|\n",
      "+--------+------+--------+-------------+\n",
      "|   40-49|Female|   White|        27920|\n",
      "|   40-49|Female|   Black|       137120|\n",
      "|   40-49|Female|   Asian|        25550|\n",
      "|   40-49|  Male|Hispanic|       107865|\n",
      "|   50-59|Female|Hispanic|        55545|\n",
      "|     60+|Female|   Asian|        19260|\n",
      "|Under 20|Female|   Black|         7150|\n",
      "|Under 20|  Male|Hispanic|         3705|\n",
      "|   20-29|Female|   Other|          330|\n",
      "|   30-39|Female|   Black|       101530|\n",
      "|   50-59|  Male|   Other|         5520|\n",
      "|   20-29|  Male|   White|        14275|\n",
      "|Under 20|  Male|   Black|         4555|\n",
      "|   50-59|  Male|Hispanic|       106310|\n",
      "|Under 20|  Male|   White|          770|\n",
      "|     60+|  Male|   Black|       169150|\n",
      "|     60+|  Male|   White|       103880|\n",
      "|   20-29|  Male|   Asian|        15370|\n",
      "|   40-49|  Male|   Other|         5865|\n",
      "|   20-29|  Male|   Other|         1380|\n",
      "+--------+------+--------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "table1_demo = final_df.groupBy(\"AgeGroup\", \"Gender\", \"Race\").agg(count(\"*\").alias(\"EmployeeCount\"))\n",
    "table1_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fa98969-610d-4d7d-98bd-b7c0915f6a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+--------+------------------+------------------+-------------------+-------------+\n",
      "|DeptCategory|AgeGroup|Gender|Race    |Avg_Engagement    |Avg_Satisfaction  |Avg_WorkLifeBalance|EmployeeCount|\n",
      "+------------+--------+------+--------+------------------+------------------+-------------------+-------------+\n",
      "|Technology  |40-49   |Female|Hispanic|3.010344827586207 |3.0620689655172413|3.0086206896551726 |2320         |\n",
      "|Corporate   |Under 20|Female|Other   |3.0272727272727273|2.6545454545454548|3.190909090909091  |330          |\n",
      "|Technology  |60+     |Male  |Hispanic|3.010344827586207 |3.0620689655172413|3.0086206896551726 |6380         |\n",
      "|Operations  |60+     |Female|White   |3.0               |3.1478260869565218|3.072463768115942  |16905        |\n",
      "|Corporate   |Under 20|Female|Asian   |3.0272727272727273|2.6545454545454548|3.190909090909091  |1320         |\n",
      "|Corporate   |60+     |Male  |White   |3.0272727272727273|2.6545454545454548|3.190909090909091  |3410         |\n",
      "|Operations  |40-49   |Male  |Other   |3.0               |3.1478260869565218|3.072463768115942  |5175         |\n",
      "|Operations  |50-59   |Male  |Black   |3.0               |3.1478260869565218|3.072463768115942  |252540       |\n",
      "|Operations  |20-29   |Female|Hispanic|3.0               |3.1478260869565218|3.072463768115942  |15870        |\n",
      "|Corporate   |50-59   |Female|Asian   |3.0272727272727273|2.6545454545454548|3.190909090909091  |4070         |\n",
      "|Operations  |30-39   |Female|White   |3.0               |3.1478260869565218|3.072463768115942  |16215        |\n",
      "|Operations  |50-59   |Male  |White   |3.0               |3.1478260869565218|3.072463768115942  |77625        |\n",
      "|Operations  |20-29   |Male  |Other   |3.0               |3.1478260869565218|3.072463768115942  |1380         |\n",
      "|Technology  |20-29   |Female|White   |3.010344827586207 |3.0620689655172413|3.0086206896551726 |1160         |\n",
      "|Technology  |60+     |Male  |Asian   |3.010344827586207 |3.0620689655172413|3.0086206896551726 |11600        |\n",
      "|Technology  |60+     |Female|Asian   |3.010344827586207 |3.0620689655172413|3.0086206896551726 |4060         |\n",
      "|Corporate   |Under 20|Male  |Hispanic|3.0272727272727273|2.6545454545454548|3.190909090909091  |1980         |\n",
      "|Operations  |30-39   |Male  |Other   |3.0               |3.1478260869565218|3.072463768115942  |4140         |\n",
      "|Technology  |60+     |Female|White   |3.010344827586207 |3.0620689655172413|3.0086206896551726 |1160         |\n",
      "|Corporate   |40-49   |Male  |Hispanic|3.0272727272727273|2.6545454545454548|3.190909090909091  |3190         |\n",
      "+------------+--------+------+--------+------------------+------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, avg, col\n",
    "table2_engagement = final_df.groupBy(\n",
    "    \"DeptCategory\", \"AgeGroup\", \"Gender\", \"Race\").agg(\n",
    "    avg(\"Engagement_Score\").alias(\"Avg_Engagement\"),\n",
    "    avg(\"Satisfaction_Score\").alias(\"Avg_Satisfaction\"),\n",
    "    avg(\"Work_Life_Balance_Score\").alias(\"Avg_WorkLifeBalance\"),\n",
    "    count(\"*\").alias(\"EmployeeCount\")\n",
    ")\n",
    "table2_engagement.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3344e31-2b22-4fd5-b880-ac9b61d5870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+--------+------------------+------------------+------------------+---------------------+-------------+\n",
      "|DeptCategory|AgeGroup|Gender|Race    |Avg_Salary        |Avg_BasePay       |Avg_OvertimePay   |Avg_TotalCompensation|Payroll_Count|\n",
      "+------------+--------+------+--------+------------------+------------------+------------------+---------------------+-------------+\n",
      "|Technology  |40-49   |Female|Hispanic|74320.25          |44631.5275000002  |0.0               |45463.00000000034    |2320         |\n",
      "|Corporate   |Under 20|Female|Other   |29120.0           |2518.8333333333335|0.0               |2518.8333333333335   |330          |\n",
      "|Technology  |60+     |Male  |Hispanic|94340.09090909091 |32295.591818180535|90.98181818181793 |61022.24636363305    |6380         |\n",
      "|Operations  |60+     |Female|White   |90946.48979591837 |41959.45081632884 |401.70265306122405|57751.68244898189    |16905        |\n",
      "|Corporate   |Under 20|Female|Asian   |29120.0           |2250.7916666666665|0.0               |2250.7916666666665   |1320         |\n",
      "|Corporate   |60+     |Male  |White   |115279.48387096774|53598.50225806457 |58.84419354838711 |65845.61193548336    |3410         |\n",
      "|Operations  |40-49   |Male  |Other   |66929.66666666667 |40173.82466666679 |3343.2060000000392|44690.511333334834   |5175         |\n",
      "|Operations  |50-59   |Male  |Black   |62466.11202185792 |35555.65228142936 |6180.311202186606 |46002.50756830835    |252540       |\n",
      "|Operations  |20-29   |Female|Hispanic|50473.586956521736|25451.02630434848 |1316.9882608695616|27659.817608696318   |15870        |\n",
      "|Corporate   |50-59   |Female|Asian   |105743.5945945946 |58195.590270270826|0.0               |59233.58567567642    |4070         |\n",
      "|Operations  |30-39   |Female|White   |78305.31914893616 |40886.57340425367 |1916.674042553216 |43932.64297872069    |16215        |\n",
      "|Operations  |50-59   |Male  |White   |95966.03111111111 |57404.74275555237 |3567.331066666576 |65170.063955552374   |77625        |\n",
      "|Operations  |20-29   |Male  |Other   |59035.5           |28323.725         |6814.122500000055 |35080.65249999956    |1380         |\n",
      "|Technology  |20-29   |Female|White   |52207.0           |2133.204999999996 |0.0               |2396.0250000000087   |1160         |\n",
      "|Technology  |60+     |Male  |Asian   |96451.7           |43570.35199999734 |176.63800000000043|52959.86899999453    |11600        |\n",
      "|Technology  |60+     |Female|Asian   |104089.42857142857|40676.522857144635|0.0               |69786.06714285411    |4060         |\n",
      "|Corporate   |Under 20|Male  |Hispanic|29120.0           |2601.472222222222 |0.0               |2601.472222222222    |1980         |\n",
      "|Operations  |30-39   |Male  |Other   |62286.916666666664|29986.424166666773|4971.844166666686 |35540.572500000584   |4140         |\n",
      "|Technology  |60+     |Female|White   |130121.0          |61191.35000000003 |0.0               |86671.28999999803    |1160         |\n",
      "|Corporate   |40-49   |Male  |Hispanic|75699.58620689655 |41696.24758620647 |253.1358620689656 |42865.78275862099    |3190         |\n",
      "+------------+--------+------+--------+------------------+------------------+------------------+---------------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "table3_compensation = final_df.groupBy(\n",
    "    \"DeptCategory\", \"AgeGroup\", \"Gender\", \"Race\").agg(\n",
    "    avg(\"Annual_Base_Salary\").alias(\"Avg_Salary\"),\n",
    "    avg(\"Base_Pay\").alias(\"Avg_BasePay\"),\n",
    "    avg(\"Overtime_Pay\").alias(\"Avg_OvertimePay\"),\n",
    "    avg(\"TotalCompensation\").alias(\"Avg_TotalCompensation\"),\n",
    "    count(\"*\").alias(\"Payroll_Count\")\n",
    ")\n",
    "table3_compensation.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03afdaf2-4116-4782-9800-fa4fdd4731aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+--------+-------------+---------------+-----------------+------------------+\n",
      "|DeptCategory|AgeGroup|Gender|Race    |Total_Leavers|Total_Employees|Avg_Tenure       |TurnoverRate      |\n",
      "+------------+--------+------+--------+-------------+---------------+-----------------+------------------+\n",
      "|Technology  |40-49   |Female|Hispanic|1240         |2320           |4.346551724137931|0.5344827586206896|\n",
      "|Corporate   |Under 20|Female|Other   |213          |330            |4.627272727272727|0.6454545454545455|\n",
      "|Technology  |60+     |Male  |Hispanic|3410         |6380           |4.346551724137931|0.5344827586206896|\n",
      "|Operations  |60+     |Female|White   |8379         |16905          |4.350724637681159|0.4956521739130435|\n",
      "|Corporate   |Under 20|Female|Asian   |852          |1320           |4.627272727272727|0.6454545454545455|\n",
      "|Corporate   |60+     |Male  |White   |2201         |3410           |4.627272727272727|0.6454545454545455|\n",
      "|Operations  |40-49   |Male  |Other   |2565         |5175           |4.350724637681159|0.4956521739130435|\n",
      "|Operations  |50-59   |Male  |Black   |125172       |252540         |4.350724637681159|0.4956521739130435|\n",
      "|Operations  |20-29   |Female|Hispanic|7866         |15870          |4.350724637681159|0.4956521739130435|\n",
      "|Corporate   |50-59   |Female|Asian   |2627         |4070           |4.627272727272727|0.6454545454545455|\n",
      "|Operations  |30-39   |Female|White   |8037         |16215          |4.350724637681159|0.4956521739130435|\n",
      "|Operations  |50-59   |Male  |White   |38475        |77625          |4.350724637681159|0.4956521739130435|\n",
      "|Operations  |20-29   |Male  |Other   |684          |1380           |4.350724637681159|0.4956521739130435|\n",
      "|Technology  |20-29   |Female|White   |620          |1160           |4.346551724137931|0.5344827586206896|\n",
      "|Technology  |60+     |Male  |Asian   |6200         |11600          |4.346551724137931|0.5344827586206896|\n",
      "|Technology  |60+     |Female|Asian   |2170         |4060           |4.346551724137931|0.5344827586206896|\n",
      "|Corporate   |Under 20|Male  |Hispanic|1278         |1980           |4.627272727272727|0.6454545454545455|\n",
      "|Operations  |30-39   |Male  |Other   |2052         |4140           |4.350724637681159|0.4956521739130435|\n",
      "|Technology  |60+     |Female|White   |620          |1160           |4.346551724137931|0.5344827586206896|\n",
      "|Corporate   |40-49   |Male  |Hispanic|2059         |3190           |4.627272727272727|0.6454545454545455|\n",
      "+------------+--------+------+--------+-------------+---------------+-----------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, avg, count\n",
    "table4_turnover = final_df.groupBy(\n",
    "    \"DeptCategory\", \"AgeGroup\", \"Gender\", \"Race\"\n",
    ").agg(\n",
    "    _sum(\"TurnoverFlag\").alias(\"Total_Leavers\"),\n",
    "    count(\"*\").alias(\"Total_Employees\"),\n",
    "    avg(\"Tenure\").alias(\"Avg_Tenure\")\n",
    ").withColumn(\n",
    "    \"TurnoverRate\", col(\"Total_Leavers\") / col(\"Total_Employees\")\n",
    ")\n",
    "table4_turnover.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ad4b019-f311-45e4-b006-f72df23b6614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+--------+------------------+--------------------+--------------+\n",
      "|DeptCategory|AgeGroup|Gender|Race    |Avg_TrainingDays  |Total_TrainingCost  |Training_Count|\n",
      "+------------+--------+------+--------+------------------+--------------------+--------------+\n",
      "|Technology  |40-49   |Female|Hispanic|3.0120689655172415|1329740.92          |2320          |\n",
      "|Corporate   |Under 20|Female|Other   |2.963636363636364 |181869.48000000019  |330           |\n",
      "|Technology  |60+     |Male  |Hispanic|3.0120689655172415|3656787.5300000035  |6380          |\n",
      "|Operations  |60+     |Female|White   |2.907246376811594 |9072758.660000052   |16905         |\n",
      "|Corporate   |Under 20|Female|Asian   |2.963636363636364 |727477.9199999997   |1320          |\n",
      "|Corporate   |60+     |Male  |White   |2.963636363636364 |1879317.959999996   |3410          |\n",
      "|Operations  |40-49   |Male  |Other   |2.907246376811594 |2777375.099999987   |5175          |\n",
      "|Operations  |50-59   |Male  |Black   |2.907246376811594 |1.3553590488000977E8|252540        |\n",
      "|Operations  |20-29   |Female|Hispanic|2.907246376811594 |8517283.640000045   |15870         |\n",
      "|Corporate   |50-59   |Female|Asian   |2.963636363636364 |2243056.9199999934  |4070          |\n",
      "|Operations  |30-39   |Female|White   |2.907246376811594 |8702441.980000045   |16215         |\n",
      "|Operations  |50-59   |Male  |White   |2.907246376811594 |4.166062649999985E7 |77625         |\n",
      "|Operations  |20-29   |Male  |Other   |2.907246376811594 |740633.3600000007   |1380          |\n",
      "|Technology  |20-29   |Female|White   |3.0120689655172415|664870.4599999998   |1160          |\n",
      "|Technology  |60+     |Male  |Asian   |3.0120689655172415|6648704.599999991   |11600         |\n",
      "|Technology  |60+     |Female|Asian   |3.0120689655172415|2327046.6100000064  |4060          |\n",
      "|Corporate   |Under 20|Male  |Hispanic|2.963636363636364 |1091216.8799999976  |1980          |\n",
      "|Operations  |30-39   |Male  |Other   |2.907246376811594 |2221900.0799999917  |4140          |\n",
      "|Technology  |60+     |Female|White   |3.0120689655172415|664870.4599999998   |1160          |\n",
      "|Corporate   |40-49   |Male  |Hispanic|2.963636363636364 |1758071.6399999978  |3190          |\n",
      "+------------+--------+------+--------+------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum as _sum, count\n",
    "table5_training = final_df.groupBy(\n",
    "    \"DeptCategory\", \"AgeGroup\", \"Gender\", \"Race\"\n",
    ").agg(\n",
    "    avg(\"Training_DurationDays\").alias(\"Avg_TrainingDays\"),\n",
    "    _sum(\"Training_Cost\").alias(\"Total_TrainingCost\"),\n",
    "    count(\"Training_Program_Name\").alias(\"Training_Count\")\n",
    ")\n",
    "table5_training.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69790976-d10f-46a1-a8f3-737fbb73ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_demo.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"analytics_output/demo\")\n",
    "table2_engagement.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"analytics_output/engagement\")\n",
    "table3_compensation.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"analytics_output/compensation\")\n",
    "table4_turnover.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"analytics_output/turnover\")\n",
    "table5_training.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"analytics_output/training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
